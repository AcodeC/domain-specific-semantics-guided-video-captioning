{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mh/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1196\n",
      "Descriptions: test=1196\n",
      "Dataset: 369\n",
      "Descriptions: test=369\n",
      "C2D: test=369\n",
      "C3D: test=369\n",
      "Semantic: test=369\n",
      "C2D: train=369\n",
      "C2D: train=369\n"
     ]
    }
   ],
   "source": [
    "#load the ids for a particular class\n",
    "filename = 'data/msvd_classes/cook_train_ID.txt'\n",
    "train= load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "\n",
    "\n",
    "# descriptions\n",
    "train_descriptions = load_descriptions('data/descriptions_processed.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "\n",
    "#load c2d features\n",
    "train_c2d_features = load_video_features('data/msvd_resnet152_features.pkl', train)\n",
    "print('C2D: train=%d' % len(train_c2d_features))\n",
    "\n",
    "#load c3d features\n",
    "train_c3d_features = load_video_features('data/msvd_c3d_features.pkl', train)\n",
    "print('C3D: train=%d' % len(train_c3d_features))\n",
    "\n",
    "#load semantic features\n",
    "train_semantic_features = load_video_features('data/msvd_semantic_features.pkl', train)\n",
    "print('Semantic: train=%d' % len(train_semantic_features))\n",
    "\n",
    "# load test set\n",
    "filename = 'data/msvd_test_ID.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "\n",
    "# descriptions\n",
    "test_descriptions = load_descriptions('data/descriptions_processed.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "\n",
    "#load c2d features\n",
    "test_c2d_features = load_video_features('data/msvd_resnet152_features.pkl', test)\n",
    "print('C2D: test=%d' % len(test_c2d_features))\n",
    "\n",
    "#load c3d features\n",
    "test_c3d_features = load_video_features('data/msvd_c3d_features.pkl', test)\n",
    "print('C3D: test=%d' % len(test_c3d_features))\n",
    "\n",
    "#load semantic features\n",
    "test_semantic_features = load_video_features('data/msvd_semantic_features.pkl', test)\n",
    "print('Semantic: test=%d' % len(test_semantic_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions\n",
      "animal\n",
      "baby\n",
      "cook\n",
      "music\n",
      "ride\n",
      "simpleactions\n"
     ]
    }
   ],
   "source": [
    "domains={'actions','animal','baby','cook','music','ride','simpleactions'}\n",
    "#load all the models\n",
    "for i in domains:\n",
    "    filename = 'domain_specific_models/'+domains[i]+'model_49.h5'\n",
    "    model[i] = load_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# given the word id return the word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# predict one word at each time step\n",
    "def generate_desc(model, tokenizer,c2d,c3d,sem, length,class_weight):\n",
    "    # intial token in the sentence\n",
    "    in_text = 'startseq'\n",
    "    # continue till either end sequence is reached or maximum length of the sentence\n",
    "    prob=0\n",
    "    for i in range(length):\n",
    "        # convert the input descriptions to their corresponding integers\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input to the maximum sequence length\n",
    "        sequence = pad_sequences([sequence], maxlen=length)    \n",
    "        #resnet152 feature\n",
    "        c2d1=np.array([c2d])\n",
    "        #c3d feature\n",
    "        c3d1=np.array([c3d])\n",
    "        #semantic feature\n",
    "        sem1=np.array([sem])\n",
    "        # predict the word probabilities\n",
    "        yhat1 = model.predict([c2d1,c3d1,sem1, sequence], verbose=0)\n",
    "        \n",
    "        yhat = argmax(yhat1)\n",
    "        prob+=yhat1[0][yhat]\n",
    "        \n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "       \n",
    "        if word is None:\n",
    "            break\n",
    "        \n",
    "        in_text += ' ' + word\n",
    "        \n",
    "        if word == 'endseq':\n",
    "            break\n",
    "        \n",
    "    return in_text,prob\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, test_c2d, test_c3d, test_sem, tokenizer, max_length, filename,all_class_weights,domain_semantics,gt_video_cls):\n",
    "    actual, predicted = list(), list()\n",
    "\n",
    "    lines = list()\n",
    "    co=1\n",
    "    #for every video generate descriptions\n",
    "    for key, desc_list in descriptions.items():\n",
    "        print(co)\n",
    "        co+=1\n",
    "        print(key)\n",
    "        prob=0\n",
    "        num_sem=0\n",
    "        gt=0\n",
    "        cls=0\n",
    "        cls1=0\n",
    "        sent='startseq'\n",
    "        sent1='startseq'\n",
    "        cls2=0\n",
    "        sent2='startseq'\n",
    "        prob1=0\n",
    "        for i in len(domains):\n",
    "            yhat,prob_new = generate_desc(model[i], tokenizer[class_map[i]], test_c2d[key], test_c3d[key], test_sem[key], max_length,all_class_weights[key])\n",
    "            #prob_new=prob_new/(len(yhat)-2)\n",
    "            prob_new=prob_new*all_class_weights[key][i]\n",
    "            \n",
    "            \n",
    "            if gt<gt_video_cls[key][i]:\n",
    "                gt=gt_video_cls[key][i]\n",
    "                cls1=i\n",
    "                sent1=yhat\n",
    "\n",
    "            \n",
    "            #analyse base on probability\n",
    "            prob_new=all_class_weights[key][i]\n",
    "            if prob1<prob_new:\n",
    "                prob1=prob_new\n",
    "                cls2=i\n",
    "                sent2=yhat\n",
    "            \n",
    "        ex=sent\n",
    "        a=sent.split('startseq')\n",
    "        b=a[1].split('endseq')\n",
    "        lines.append('beam_size_1'+'\\t'+key + '\\t' + b[0])\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(sent.split())\n",
    "        #print(sent)\n",
    "        #\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    bleu=np.zeros(4)\n",
    "    # calculate BLEU score\n",
    "    bleu[0]=corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
    "    bleu[1]=corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu[2]=corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n",
    "    bleu[3]=corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    print('BLEU-1: %f' % bleu[0])\n",
    "    print('BLEU-2: %f' % bleu[1])\n",
    "    print('BLEU-3: %f' % bleu[2])\n",
    "    print('BLEU-4: %f' % bleu[3])\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=49\n",
    "filename_new = 'generated_captions.txt'\n",
    "evaluate_model(model, test_descriptions,test_c2d_features, test_c3d_features, test_semantic_features, all_tokenizer, max_length,filename_new,class_features,domain_semantics,gt_video_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
